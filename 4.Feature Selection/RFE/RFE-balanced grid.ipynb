{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "111cb8fc-f829-4ade-8a39-01f24022a3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44573de4-751b-431e-a277-5a299285c518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfeFeature(indep_X,dep_Y,n):\n",
    "        rfelist=[]\n",
    "        \n",
    "        log_model = LogisticRegression(solver='lbfgs', class_weight='balanced')\n",
    "        RF = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0,class_weight='balanced')\n",
    "        # NB = GaussianNB()\n",
    "        DT= DecisionTreeClassifier(criterion = 'gini', max_features='sqrt',splitter='best',random_state = 0,class_weight='balanced')\n",
    "        from collections import Counter\n",
    "        #counter = Counter(y_train)  # Count class occurrences\n",
    "        #ratio = counter[0] / counter[1]\n",
    "        XGB= XGBClassifier(objective='binary:logistic',max_depth= 5,learning_rate=0.1,subsample=1,n_estimators=100)#,scale_pos_weight=ratio)\n",
    "        #svc_model = SVC(kernel = 'linear', random_state = 0)\n",
    "        #knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "        rfemodellist=[log_model,RF,DT,XGB] \n",
    "        for i in   rfemodellist:\n",
    "            print(i)\n",
    "            log_rfe = RFE(i, n_features_to_select=n)\n",
    "            log_fit = log_rfe.fit(indep_X, dep_Y)\n",
    "            log_rfe_feature=log_fit.transform(indep_X)\n",
    "            rfelist.append(log_rfe_feature)\n",
    "        return rfelist\n",
    "    \n",
    "\n",
    "def split_scalar(indep_X,dep_Y):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(indep_X, dep_Y, test_size = 0.2, random_state = 0)\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(indep_X,dep_Y, test_size = 0.2, random_state = 0)\n",
    "        \n",
    "        #Feature Scaling\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        sc = StandardScaler()\n",
    "        X_train = sc.fit_transform(X_train)\n",
    "        X_test = sc.transform(X_test)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "def cm_prediction(classifier,X_test):\n",
    "     y_pred = classifier.predict(X_test)\n",
    "        \n",
    "        # Making the Confusion Matrix\n",
    "     from sklearn.metrics import confusion_matrix\n",
    "     cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "     from sklearn.metrics import accuracy_score \n",
    "     from sklearn.metrics import classification_report \n",
    "        #from sklearn.metrics import confusion_matrix\n",
    "        #cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "     Accuracy=accuracy_score(y_test, y_pred )\n",
    "        \n",
    "     report=classification_report(y_test, y_pred)\n",
    "     return  classifier,Accuracy,report,X_test,y_test,cm\n",
    "\n",
    "def logistic(X_train,y_train,X_test):       \n",
    "        # Fitting K-NN to the Training set\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        classifier = LogisticRegression(random_state = 0,class_weight='balanced')\n",
    "        classifier.fit(X_train, y_train)\n",
    "        classifier,Accuracy,report,X_test,y_test,cm=cm_prediction(classifier,X_test)\n",
    "        return  classifier,Accuracy,report,X_test,y_test,cm      \n",
    "    \n",
    "def Decision(X_train,y_train,X_test):\n",
    "        \n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0,class_weight='balanced')\n",
    "        classifier.fit(X_train, y_train)\n",
    "        classifier,Accuracy,report,X_test,y_test,cm=cm_prediction(classifier,X_test)\n",
    "        return  classifier,Accuracy,report,X_test,y_test,cm      \n",
    "\n",
    "\n",
    "def random(X_train,y_train,X_test):\n",
    "        \n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0,class_weight='balanced')\n",
    "        classifier.fit(X_train, y_train)\n",
    "        classifier,Accuracy,report,X_test,y_test,cm=cm_prediction(classifier,X_test)\n",
    "        return  classifier,Accuracy,report,X_test,y_test,cm\n",
    "\n",
    "def Xgboost(X_train,y_train,X_test):\n",
    "        import xgboost\n",
    "        from xgboost import XGBClassifier\n",
    "        from collections import Counter\n",
    "        counter = Counter(y_train)  # Count class occurrences\n",
    "        ratio = counter[0] / counter[1]\n",
    "        classifier = XGBClassifier(objective='binary:logistic',max_depth= 5,learning_rate=0.1,subsample=1,n_estimators=100,scale_pos_weight=ratio)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        classifier,Accuracy,report,X_test,y_test,cm=cm_prediction(classifier,X_test)\n",
    "        return  classifier,Accuracy,report,X_test,y_test,cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a876b56b-080e-4e01-aa5f-5f955b7f26b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99c0ef75-0cb9-497a-9900-e74693cff40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= pd.read_csv(\"Preprocessed_dataset.csv\")\n",
    "dataset1=pd.get_dummies(dataset,drop_first=True,dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca060d8d-e087-432b-97fe-c1714276cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "indep_x= dataset1.drop('Conversion',axis=1)\n",
    "dep_y=dataset1['Conversion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dfecf75-34a3-45e3-9cdb-4cba2e5dd0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(class_weight='balanced')\n",
      "RandomForestClassifier(class_weight='balanced', criterion='entropy',\n",
      "                       n_estimators=10, random_state=0)\n",
      "DecisionTreeClassifier(class_weight='balanced', max_features='sqrt',\n",
      "                       random_state=0)\n",
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[0.04391851, 0.08803141, 2.39901653, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.15572507, 0.18272468, 2.91713775, ..., 0.        , 0.        ,\n",
       "         1.        ],\n",
       "        [0.27749037, 0.07642272, 8.2236191 , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.05652592, 0.13382612, 2.85324058, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.02396129, 0.13838618, 1.00296447, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.1856701 , 0.05722808, 6.96473936, ..., 0.        , 0.        ,\n",
       "         0.        ]]),\n",
       " array([[1.36912000e+05, 6.49787007e+03, 4.39185107e-02, ...,\n",
       "         9.00000000e+00, 4.00000000e+00, 6.88000000e+02],\n",
       "        [4.17600000e+04, 3.89866861e+03, 1.55725071e-01, ...,\n",
       "         7.00000000e+00, 2.00000000e+00, 3.45900000e+03],\n",
       "        [8.84560000e+04, 1.54642960e+03, 2.77490369e-01, ...,\n",
       "         2.00000000e+00, 8.00000000e+00, 2.33700000e+03],\n",
       "        ...,\n",
       "        [1.25471000e+05, 4.60953464e+03, 5.65259173e-02, ...,\n",
       "         0.00000000e+00, 3.00000000e+00, 7.38000000e+02],\n",
       "        [1.07862000e+05, 9.47610635e+03, 2.39612921e-02, ...,\n",
       "         5.00000000e+00, 7.00000000e+00, 2.70900000e+03],\n",
       "        [9.30020000e+04, 7.74362707e+03, 1.85670096e-01, ...,\n",
       "         9.00000000e+00, 9.00000000e+00, 3.41000000e+02]]),\n",
       " array([[5.60000000e+01, 1.36912000e+05, 6.49787007e+03, ...,\n",
       "         9.00000000e+00, 4.00000000e+00, 6.88000000e+02],\n",
       "        [6.90000000e+01, 4.17600000e+04, 3.89866861e+03, ...,\n",
       "         7.00000000e+00, 2.00000000e+00, 3.45900000e+03],\n",
       "        [4.60000000e+01, 8.84560000e+04, 1.54642960e+03, ...,\n",
       "         2.00000000e+00, 8.00000000e+00, 2.33700000e+03],\n",
       "        ...,\n",
       "        [2.80000000e+01, 1.25471000e+05, 4.60953464e+03, ...,\n",
       "         0.00000000e+00, 3.00000000e+00, 7.38000000e+02],\n",
       "        [1.90000000e+01, 1.07862000e+05, 9.47610635e+03, ...,\n",
       "         5.00000000e+00, 7.00000000e+00, 2.70900000e+03],\n",
       "        [3.10000000e+01, 9.30020000e+04, 7.74362707e+03, ...,\n",
       "         9.00000000e+00, 9.00000000e+00, 3.41000000e+02]]),\n",
       " array([[6.49787007e+03, 4.39185107e-02, 8.80314121e-02, ...,\n",
       "         6.88000000e+02, 0.00000000e+00, 0.00000000e+00],\n",
       "        [3.89866861e+03, 1.55725071e-01, 1.82724683e-01, ...,\n",
       "         3.45900000e+03, 1.00000000e+00, 0.00000000e+00],\n",
       "        [1.54642960e+03, 2.77490369e-01, 7.64227201e-02, ...,\n",
       "         2.33700000e+03, 0.00000000e+00, 0.00000000e+00],\n",
       "        ...,\n",
       "        [4.60953464e+03, 5.65259173e-02, 1.33826123e-01, ...,\n",
       "         7.38000000e+02, 0.00000000e+00, 0.00000000e+00],\n",
       "        [9.47610635e+03, 2.39612921e-02, 1.38386181e-01, ...,\n",
       "         2.70900000e+03, 0.00000000e+00, 0.00000000e+00],\n",
       "        [7.74362707e+03, 1.85670096e-01, 5.72280791e-02, ...,\n",
       "         3.41000000e+02, 0.00000000e+00, 0.00000000e+00]])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfelist=rfeFeature(indep_x,dep_y,12)\n",
    "rfelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5268b3ee-5557-4f40-831c-2395fb176d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification Report:\n",
      "[[140  54]\n",
      " [426 980]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.72      0.37       194\n",
      "           1       0.95      0.70      0.80      1406\n",
      "\n",
      "    accuracy                           0.70      1600\n",
      "   macro avg       0.60      0.71      0.59      1600\n",
      "weighted avg       0.86      0.70      0.75      1600\n",
      "\n",
      "\n",
      " Decision Tree Classification Report:\n",
      "[[  65  129]\n",
      " [ 143 1263]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.34      0.32       194\n",
      "           1       0.91      0.90      0.90      1406\n",
      "\n",
      "    accuracy                           0.83      1600\n",
      "   macro avg       0.61      0.62      0.61      1600\n",
      "weighted avg       0.84      0.83      0.83      1600\n",
      "\n",
      "\n",
      " Random Forest Classification Report:\n",
      "[[  42  152]\n",
      " [  39 1367]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.22      0.31       194\n",
      "           1       0.90      0.97      0.93      1406\n",
      "\n",
      "    accuracy                           0.88      1600\n",
      "   macro avg       0.71      0.59      0.62      1600\n",
      "weighted avg       0.85      0.88      0.86      1600\n",
      "\n",
      "\n",
      " XgboostClassification Report:\n",
      "[[ 119   75]\n",
      " [ 209 1197]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.61      0.46       194\n",
      "           1       0.94      0.85      0.89      1406\n",
      "\n",
      "    accuracy                           0.82      1600\n",
      "   macro avg       0.65      0.73      0.67      1600\n",
      "weighted avg       0.87      0.82      0.84      1600\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "[[ 142   52]\n",
      " [ 382 1024]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.73      0.40       194\n",
      "           1       0.95      0.73      0.83      1406\n",
      "\n",
      "    accuracy                           0.73      1600\n",
      "   macro avg       0.61      0.73      0.61      1600\n",
      "weighted avg       0.87      0.73      0.77      1600\n",
      "\n",
      "\n",
      " Decision Tree Classification Report:\n",
      "[[  71  123]\n",
      " [ 127 1279]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.37      0.36       194\n",
      "           1       0.91      0.91      0.91      1406\n",
      "\n",
      "    accuracy                           0.84      1600\n",
      "   macro avg       0.64      0.64      0.64      1600\n",
      "weighted avg       0.85      0.84      0.84      1600\n",
      "\n",
      "\n",
      " Random Forest Classification Report:\n",
      "[[  54  140]\n",
      " [  31 1375]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.28      0.39       194\n",
      "           1       0.91      0.98      0.94      1406\n",
      "\n",
      "    accuracy                           0.89      1600\n",
      "   macro avg       0.77      0.63      0.66      1600\n",
      "weighted avg       0.87      0.89      0.87      1600\n",
      "\n",
      "\n",
      " XgboostClassification Report:\n",
      "[[ 129   65]\n",
      " [ 154 1252]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.66      0.54       194\n",
      "           1       0.95      0.89      0.92      1406\n",
      "\n",
      "    accuracy                           0.86      1600\n",
      "   macro avg       0.70      0.78      0.73      1600\n",
      "weighted avg       0.89      0.86      0.87      1600\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "[[ 144   50]\n",
      " [ 382 1024]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.74      0.40       194\n",
      "           1       0.95      0.73      0.83      1406\n",
      "\n",
      "    accuracy                           0.73      1600\n",
      "   macro avg       0.61      0.74      0.61      1600\n",
      "weighted avg       0.87      0.73      0.77      1600\n",
      "\n",
      "\n",
      " Decision Tree Classification Report:\n",
      "[[  73  121]\n",
      " [ 138 1268]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.38      0.36       194\n",
      "           1       0.91      0.90      0.91      1406\n",
      "\n",
      "    accuracy                           0.84      1600\n",
      "   macro avg       0.63      0.64      0.63      1600\n",
      "weighted avg       0.84      0.84      0.84      1600\n",
      "\n",
      "\n",
      " Random Forest Classification Report:\n",
      "[[  47  147]\n",
      " [  31 1375]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.24      0.35       194\n",
      "           1       0.90      0.98      0.94      1406\n",
      "\n",
      "    accuracy                           0.89      1600\n",
      "   macro avg       0.75      0.61      0.64      1600\n",
      "weighted avg       0.87      0.89      0.87      1600\n",
      "\n",
      "\n",
      " XgboostClassification Report:\n",
      "[[ 127   67]\n",
      " [ 144 1262]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.65      0.55       194\n",
      "           1       0.95      0.90      0.92      1406\n",
      "\n",
      "    accuracy                           0.87      1600\n",
      "   macro avg       0.71      0.78      0.73      1600\n",
      "weighted avg       0.89      0.87      0.88      1600\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "[[ 144   50]\n",
      " [ 389 1017]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.74      0.40       194\n",
      "           1       0.95      0.72      0.82      1406\n",
      "\n",
      "    accuracy                           0.73      1600\n",
      "   macro avg       0.61      0.73      0.61      1600\n",
      "weighted avg       0.87      0.73      0.77      1600\n",
      "\n",
      "\n",
      " Decision Tree Classification Report:\n",
      "[[  82  112]\n",
      " [ 136 1270]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.42      0.40       194\n",
      "           1       0.92      0.90      0.91      1406\n",
      "\n",
      "    accuracy                           0.84      1600\n",
      "   macro avg       0.65      0.66      0.65      1600\n",
      "weighted avg       0.85      0.84      0.85      1600\n",
      "\n",
      "\n",
      " Random Forest Classification Report:\n",
      "[[  53  141]\n",
      " [  21 1385]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.27      0.40       194\n",
      "           1       0.91      0.99      0.94      1406\n",
      "\n",
      "    accuracy                           0.90      1600\n",
      "   macro avg       0.81      0.63      0.67      1600\n",
      "weighted avg       0.88      0.90      0.88      1600\n",
      "\n",
      "\n",
      " XgboostClassification Report:\n",
      "[[ 124   70]\n",
      " [ 145 1261]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.64      0.54       194\n",
      "           1       0.95      0.90      0.92      1406\n",
      "\n",
      "    accuracy                           0.87      1600\n",
      "   macro avg       0.70      0.77      0.73      1600\n",
      "weighted avg       0.89      0.87      0.87      1600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#12\n",
    "for i in rfelist:   \n",
    "    X_train, X_test, y_train, y_test=split_scalar(i,dep_y)   \n",
    "    \n",
    "        \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=logistic(X_train,y_train,X_test)\n",
    "    print(\"Logistic Regression Classification Report:\")\n",
    "    print(cm)\n",
    "    print(report)\n",
    "    \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=Decision(X_train,y_train,X_test)\n",
    "    print(\"\\n Decision Tree Classification Report:\")\n",
    "    print(cm)\n",
    "    print(report)\n",
    "    \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=random(X_train,y_train,X_test)\n",
    "    print(\"\\n Random Forest Classification Report:\")\n",
    "    print(cm)\n",
    "    print(report)\n",
    "    \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=Xgboost(X_train,y_train,X_test)\n",
    "    print(\"\\n XgboostClassification Report:\")\n",
    "    print(cm)\n",
    "    print(report)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "274809b6-b00c-439f-a535-e296808aff5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[140  54]\n",
      " [445 961]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.72      0.36       194\n",
      "           1       0.95      0.68      0.79      1406\n",
      "\n",
      "    accuracy                           0.69      1600\n",
      "   macro avg       0.59      0.70      0.58      1600\n",
      "weighted avg       0.86      0.69      0.74      1600\n",
      "\n",
      "[[  50  144]\n",
      " [ 135 1271]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.26      0.26       194\n",
      "           1       0.90      0.90      0.90      1406\n",
      "\n",
      "    accuracy                           0.83      1600\n",
      "   macro avg       0.58      0.58      0.58      1600\n",
      "weighted avg       0.82      0.83      0.82      1600\n",
      "\n",
      "[[  40  154]\n",
      " [  34 1372]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.21      0.30       194\n",
      "           1       0.90      0.98      0.94      1406\n",
      "\n",
      "    accuracy                           0.88      1600\n",
      "   macro avg       0.72      0.59      0.62      1600\n",
      "weighted avg       0.86      0.88      0.86      1600\n",
      "\n",
      "[[ 110   84]\n",
      " [ 235 1171]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.57      0.41       194\n",
      "           1       0.93      0.83      0.88      1406\n",
      "\n",
      "    accuracy                           0.80      1600\n",
      "   macro avg       0.63      0.70      0.64      1600\n",
      "weighted avg       0.86      0.80      0.82      1600\n",
      "\n",
      "[[139  55]\n",
      " [431 975]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.72      0.36       194\n",
      "           1       0.95      0.69      0.80      1406\n",
      "\n",
      "    accuracy                           0.70      1600\n",
      "   macro avg       0.60      0.70      0.58      1600\n",
      "weighted avg       0.86      0.70      0.75      1600\n",
      "\n",
      "[[  76  118]\n",
      " [ 124 1282]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.39      0.39       194\n",
      "           1       0.92      0.91      0.91      1406\n",
      "\n",
      "    accuracy                           0.85      1600\n",
      "   macro avg       0.65      0.65      0.65      1600\n",
      "weighted avg       0.85      0.85      0.85      1600\n",
      "\n",
      "[[  52  142]\n",
      " [  34 1372]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.27      0.37       194\n",
      "           1       0.91      0.98      0.94      1406\n",
      "\n",
      "    accuracy                           0.89      1600\n",
      "   macro avg       0.76      0.62      0.66      1600\n",
      "weighted avg       0.87      0.89      0.87      1600\n",
      "\n",
      "[[ 119   75]\n",
      " [ 170 1236]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.61      0.49       194\n",
      "           1       0.94      0.88      0.91      1406\n",
      "\n",
      "    accuracy                           0.85      1600\n",
      "   macro avg       0.68      0.75      0.70      1600\n",
      "weighted avg       0.88      0.85      0.86      1600\n",
      "\n",
      "[[ 143   51]\n",
      " [ 377 1029]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.74      0.40       194\n",
      "           1       0.95      0.73      0.83      1406\n",
      "\n",
      "    accuracy                           0.73      1600\n",
      "   macro avg       0.61      0.73      0.61      1600\n",
      "weighted avg       0.87      0.73      0.78      1600\n",
      "\n",
      "[[  75  119]\n",
      " [ 132 1274]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.39      0.37       194\n",
      "           1       0.91      0.91      0.91      1406\n",
      "\n",
      "    accuracy                           0.84      1600\n",
      "   macro avg       0.64      0.65      0.64      1600\n",
      "weighted avg       0.85      0.84      0.85      1600\n",
      "\n",
      "[[  62  132]\n",
      " [  25 1381]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.32      0.44       194\n",
      "           1       0.91      0.98      0.95      1406\n",
      "\n",
      "    accuracy                           0.90      1600\n",
      "   macro avg       0.81      0.65      0.69      1600\n",
      "weighted avg       0.89      0.90      0.88      1600\n",
      "\n",
      "[[ 124   70]\n",
      " [ 161 1245]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.64      0.52       194\n",
      "           1       0.95      0.89      0.92      1406\n",
      "\n",
      "    accuracy                           0.86      1600\n",
      "   macro avg       0.69      0.76      0.72      1600\n",
      "weighted avg       0.88      0.86      0.87      1600\n",
      "\n",
      "[[ 142   52]\n",
      " [ 394 1012]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.73      0.39       194\n",
      "           1       0.95      0.72      0.82      1406\n",
      "\n",
      "    accuracy                           0.72      1600\n",
      "   macro avg       0.61      0.73      0.60      1600\n",
      "weighted avg       0.87      0.72      0.77      1600\n",
      "\n",
      "[[  68  126]\n",
      " [ 140 1266]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.35      0.34       194\n",
      "           1       0.91      0.90      0.90      1406\n",
      "\n",
      "    accuracy                           0.83      1600\n",
      "   macro avg       0.62      0.63      0.62      1600\n",
      "weighted avg       0.84      0.83      0.84      1600\n",
      "\n",
      "[[  58  136]\n",
      " [  33 1373]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.30      0.41       194\n",
      "           1       0.91      0.98      0.94      1406\n",
      "\n",
      "    accuracy                           0.89      1600\n",
      "   macro avg       0.77      0.64      0.67      1600\n",
      "weighted avg       0.88      0.89      0.88      1600\n",
      "\n",
      "[[ 126   68]\n",
      " [ 154 1252]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.65      0.53       194\n",
      "           1       0.95      0.89      0.92      1406\n",
      "\n",
      "    accuracy                           0.86      1600\n",
      "   macro avg       0.70      0.77      0.73      1600\n",
      "weighted avg       0.89      0.86      0.87      1600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#10\n",
    "for i in rfelist:   \n",
    "    X_train, X_test, y_train, y_test=split_scalar(i,dep_y)   \n",
    "    \n",
    "        \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=logistic(X_train,y_train,X_test)\n",
    "    print(cm)\n",
    "    print(report)\n",
    "    \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=Decision(X_train,y_train,X_test)\n",
    "    print(cm)\n",
    "    print(report)\n",
    "    \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=random(X_train,y_train,X_test)\n",
    "    print(cm)\n",
    "    print(report)\n",
    "    \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=Xgboost(X_train,y_train,X_test)\n",
    "    print(cm)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dce528b-c861-4f29-85d3-2ca197f16753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
