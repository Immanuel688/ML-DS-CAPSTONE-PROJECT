{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "111cb8fc-f829-4ade-8a39-01f24022a3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be439bc-d4d3-4ffa-8371-e0b0df1a48d4",
   "metadata": {},
   "source": [
    "Even though refit=True fits the best model, GridSearchCV itself is not a model—it’s just a wrapper around multiple models tested with different hyperparameters.\n",
    "\n",
    "GridSearchCV does not have coef_ or feature_importances_ (which RFE needs).\n",
    "The actual trained model with the best parameters is stored inside best_estimator_.\n",
    "If you try to use GridSearchCV directly in RFE, it won’t work because RFE expects a model, not a hyperparameter search object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b9643b-463f-404b-8a9b-8445a79423d4",
   "metadata": {},
   "source": [
    "def rfeFeature(indep_X, dep_Y, n):\n",
    "    rfelist = []\n",
    "    \n",
    "    # Define parameter grids for different models\n",
    "    param_grid_log = {\n",
    "        'C': [0.01, 0.1, 1, 10, 100], \n",
    "        'penalty': ['l1', 'l2'], \n",
    "        'solver': ['liblinear', 'saga', 'lbfgs'], \n",
    "        'max_iter': [50, 100]\n",
    "    }\n",
    "    \n",
    "    param_grid_RF = {\n",
    "        'criterion': [\"gini\", \"entropy\", \"log_loss\"], \n",
    "        'max_features': [None, \"sqrt\", \"log2\"], \n",
    "        'n_estimators': [10, 50]\n",
    "    }\n",
    "    \n",
    "    param_grid_DT = {\n",
    "        'criterion': [\"gini\", \"entropy\", \"log_loss\"], \n",
    "        'splitter': [\"best\", \"random\"], \n",
    "        'max_features': [None, \"sqrt\", \"log2\"]\n",
    "    }\n",
    "    \n",
    "    param_grid_XGB = {\n",
    "        'n_estimators': [100, 300], \n",
    "        'max_depth': [3, 6], \n",
    "        'learning_rate': [0.01, 0.1, 0.3], \n",
    "        'subsample': [0.8, 1.0], \n",
    "        'colsample_bytree': [0.7, 1.0], \n",
    "        'gamma': [1, 5]\n",
    "    }\n",
    "    \n",
    "    # Create GridSearchCV objects for each model\n",
    "    grid_log = GridSearchCV(LogisticRegression(random_state=42), param_grid_log, cv=5, scoring='f1', n_jobs=-1, refit=True)\n",
    "    grid_RF = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_RF, cv=5, scoring='f1', n_jobs=-1, refit=True, verbose=3)\n",
    "    grid_DT = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid_DT, cv=5, scoring='f1', n_jobs=-1, refit=True, verbose=3)\n",
    "    grid_XGB = GridSearchCV(XGBClassifier(random_state=42), param_grid_XGB, cv=5, scoring='f1', n_jobs=-1, refit=True, verbose=3)\n",
    "    \n",
    "    # List of models\n",
    "    rfemodellist = [grid_log, grid_RF, grid_DT, grid_XGB]\n",
    "\n",
    "    for grid in rfemodellist:\n",
    "        print(f\"Fitting GridSearchCV for {grid.estimator.__class__.__name__}...\")\n",
    "        \n",
    "        # Fit GridSearchCV\n",
    "        grid.fit(indep_X, dep_Y)\n",
    "\n",
    "        # Get the best model\n",
    "        best_model = grid.best_estimator_\n",
    "\n",
    "        # Apply RFE using the best model\n",
    "        log_rfe = RFE(best_model, n_features_to_select=n)\n",
    "        log_fit = log_rfe.fit(indep_X, dep_Y)\n",
    "        log_rfe_feature = log_fit.transform(indep_X)\n",
    "        \n",
    "        rfelist.append(log_rfe_feature)\n",
    "\n",
    "    return rfelist\n",
    "\n",
    "\n",
    "def rfeFeature(indep_X,dep_Y,n):\n",
    "        rfelist=[]\n",
    "        \n",
    "        param_grid_log = {'C': [0.01, 0.1, 1, 10,100], 'penalty': ['l1', 'l2'], 'solver': ['liblinear', 'saga','lbfgs'],'max_iter': [50,100]}\n",
    "        grid_log= GridSearchCV(LogisticRegression(), param_grid_log, cv=5, scoring='f1', n_jobs=-1,refit=True)\n",
    "        param_grid_RF={'criterion':[\"gini\", \"entropy\", \"log_loss\"],'max_features': [None,\"sqrt\", \"log2\"], 'n_estimators':[10,50]}\n",
    "        grid_RF= GridSearchCV(RandomForestClassifier(), param_grid_RF, refit= True, verbose=3, n_jobs=-1, scoring= 'f1' )\n",
    "        # NB = GaussianNB()\n",
    "        param_grid_DT={'criterion':[\"gini\", \"entropy\", \"log_loss\"], 'splitter': [\"best\", \"random\"],'max_features': [None,\"sqrt\", \"log2\"]}\n",
    "        grid_DT= GridSearchCV(DecisionTreeClassifier(), param_grid_DT, refit= True, verbose=3, n_jobs=-1, scoring= 'f1' )\n",
    "        from collections import Counter\n",
    "        #counter = Counter(y_train)  # Count class occurrences\n",
    "        #ratio = counter[0] / counter[1]\n",
    "        param_grid_XGB = {'n_estimators': [100, 300],'max_depth': [3, 6],'learning_rate': [0.01, 0.1, 0.3],'subsample': [ 0.8, 1.0],'colsample_bytree': [0.7, 1.0],'gamma': [1, 5]}\n",
    "        grid_XGB = GridSearchCV(XGBClassifier(), param_grid_XGB, refit= True, verbose=3, n_jobs=-1, scoring= 'accuracy' )\n",
    "        #svc_model = SVC(kernel = 'linear', random_state = 0)\n",
    "        #knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "        rfemodellist=[grid_log,grid_RF,grid_DT,grid_XGB] \n",
    "        for i in   rfemodellist:\n",
    "            print(i)\n",
    "            log_rfe = RFE(i.best_estimator_, n_features_to_select=n)\n",
    "            log_fit = log_rfe.fit(indep_X, dep_Y)\n",
    "            log_rfe_feature=log_fit.transform(indep_X)\n",
    "            rfelist.append(log_rfe_feature)\n",
    "        return rfelist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44573de4-751b-431e-a277-5a299285c518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfeFeature(indep_X, dep_Y, n):\n",
    "    rfelist = []\n",
    "    \n",
    "    # Define parameter grids for different models\n",
    "    param_grid_log = {\n",
    "        'C': [0.01, 0.1, 1, 10, 100], \n",
    "        'penalty': ['l1', 'l2'], \n",
    "        'solver': ['liblinear', 'saga', 'lbfgs'], \n",
    "        'max_iter': [50, 100]\n",
    "    }\n",
    "    \n",
    "    param_grid_RF = {\n",
    "        'criterion': [\"gini\", \"entropy\", \"log_loss\"], \n",
    "        'max_features': [None, \"sqrt\", \"log2\"], \n",
    "        'n_estimators': [10, 50]\n",
    "    }\n",
    "    \n",
    "    param_grid_DT = {\n",
    "        'criterion': [\"gini\", \"entropy\", \"log_loss\"], \n",
    "        'splitter': [\"best\", \"random\"], \n",
    "        'max_features': [None, \"sqrt\", \"log2\"]\n",
    "    }\n",
    "    \n",
    "    param_grid_XGB = {\n",
    "        'n_estimators': [100, 300], \n",
    "        'max_depth': [3, 6], \n",
    "        'learning_rate': [0.01, 0.1, 0.3], \n",
    "        'subsample': [0.8, 1.0], \n",
    "        'colsample_bytree': [0.7, 1.0], \n",
    "        'gamma': [1, 5]\n",
    "    }\n",
    "    \n",
    "    # Create GridSearchCV objects for each model\n",
    "    grid_log = GridSearchCV(LogisticRegression(random_state=42), param_grid_log, cv=5, scoring='f1', n_jobs=-1, refit=True)\n",
    "    grid_RF = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_RF, cv=5, scoring='f1', n_jobs=-1, refit=True, verbose=3)\n",
    "    grid_DT = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid_DT, cv=5, scoring='f1', n_jobs=-1, refit=True, verbose=3)\n",
    "    grid_XGB = GridSearchCV(XGBClassifier(random_state=42), param_grid_XGB, cv=5, scoring='f1', n_jobs=-1, refit=True, verbose=3)\n",
    "    \n",
    "    # List of models\n",
    "    rfemodellist = [grid_log, grid_RF, grid_DT, grid_XGB]\n",
    "\n",
    "    for grid in rfemodellist:\n",
    "        print(f\"Fitting GridSearchCV for {grid.estimator.__class__.__name__}...\")\n",
    "        \n",
    "        # Fit GridSearchCV\n",
    "        grid.fit(indep_X, dep_Y)\n",
    "\n",
    "        # Get the best model\n",
    "        best_model = grid.best_estimator_\n",
    "\n",
    "        # Apply RFE using the best model\n",
    "        log_rfe = RFE(best_model, n_features_to_select=n)\n",
    "        log_fit = log_rfe.fit(indep_X, dep_Y)\n",
    "        log_rfe_feature = log_fit.transform(indep_X)\n",
    "        \n",
    "        rfelist.append(log_rfe_feature)\n",
    "\n",
    "    return rfelist\n",
    "    \n",
    "\n",
    "def split_scalar(indep_X,dep_Y):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(indep_X, dep_Y, test_size = 0.2, random_state = 0)\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(indep_X,dep_Y, test_size = 0.2, random_state = 0)\n",
    "        \n",
    "        #Feature Scaling\n",
    "        #from sklearn.preprocessing import StandardScaler\n",
    "        #sc = StandardScaler()\n",
    "        #X_train = sc.fit_transform(X_train)\n",
    "        #X_test = sc.transform(X_test)\n",
    "        #from imblearn.over_sampling import RandomOverSampler\n",
    "        #ros = RandomOverSampler(random_state=42)\n",
    "        #X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "                \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "def cm_prediction(classifier,X_test):\n",
    "     y_pred = classifier.predict(X_test)\n",
    "        \n",
    "        # Making the Confusion Matrix\n",
    "     from sklearn.metrics import confusion_matrix\n",
    "     cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "     from sklearn.metrics import accuracy_score \n",
    "     from sklearn.metrics import classification_report \n",
    "        #from sklearn.metrics import confusion_matrix\n",
    "        #cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "     Accuracy=accuracy_score(y_test, y_pred )\n",
    "        \n",
    "     report=classification_report(y_test, y_pred)\n",
    "     return  classifier,Accuracy,report,X_test,y_test,cm\n",
    "\n",
    "def logistic(X_train_resampled,y_train_resampled,X_test):       \n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        param_grid = {'C': [0.01, 0.1, 1, 10,100], 'penalty': ['l1', 'l2'], 'solver': ['liblinear', 'saga','lbfgs'],'max_iter': [50,100],'random_state':[42]}\n",
    "        grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='f1', n_jobs=-1,refit=True)\n",
    "        grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "        grid_search,Accuracy,report,X_test,y_test,cm=cm_prediction(grid_search,X_test)\n",
    "        return  grid_search,Accuracy,report,X_test,y_test,cm \n",
    "\n",
    "def Decision(X_train_resampled,y_train_resampled,X_test):\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        param_grid={'criterion':[\"gini\", \"entropy\", \"log_loss\"], 'splitter': [\"best\", \"random\"],'max_features': [None,\"sqrt\", \"log2\"], 'random_state':[42]}\n",
    "        grid_search= GridSearchCV(DecisionTreeClassifier(), param_grid, refit= True, verbose=3, n_jobs=-1, scoring= 'f1' )\n",
    "        grid_search.fit(X_train_resampled,y_train_resampled)\n",
    "        grid_search,Accuracy,report,X_test,y_test,cm=cm_prediction(grid_search,X_test)\n",
    "        return  grid_search,Accuracy,report,X_test,y_test,cm\n",
    "\n",
    "def random(X_train_resampled,y_train_resampled,X_test):\n",
    "        \n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        param_grid={'criterion':[\"gini\", \"entropy\", \"log_loss\"],'max_features': [None,\"sqrt\", \"log2\"], 'n_estimators':[10,50,100],'random_state':[42]}\n",
    "        grid_search= GridSearchCV(RandomForestClassifier(), param_grid, refit= True, verbose=3, n_jobs=-1, scoring= 'f1' )\n",
    "        grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "        grid_search,Accuracy,report,X_test,y_test,cm=cm_prediction(grid_search,X_test)\n",
    "        return  grid_search,Accuracy,report,X_test,y_test,cm\n",
    "\n",
    "def Xgboost(X_train_resampled,y_train_resampled,X_test):\n",
    "        from xgboost import XGBClassifier\n",
    "        from collections import Counter\n",
    "        counter = Counter(y_train)  # Count class occurrences\n",
    "        ratio = counter[0] / counter[1]\n",
    "        param_grid = {'n_estimators': [100, 300],'max_depth': [3, 6],'learning_rate': [0.01, 0.1, 0.3],'subsample': [ 0.8, 1.0],'colsample_bytree': [0.7, 1.0],'gamma': [1, 5]}\n",
    "        grid_search = GridSearchCV(XGBClassifier(), param_grid, refit= True, verbose=3, n_jobs=-1, scoring= 'f1' )\n",
    "        grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "        grid_search,Accuracy,report,X_test,y_test,cm=cm_prediction(grid_search,X_test)\n",
    "        return  grid_search,Accuracy,report,X_test,y_test,cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a876b56b-080e-4e01-aa5f-5f955b7f26b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99c0ef75-0cb9-497a-9900-e74693cff40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= pd.read_csv(\"Preprocessed_dataset.csv\")\n",
    "dataset1=pd.get_dummies(dataset,drop_first=True,dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca060d8d-e087-432b-97fe-c1714276cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "indep_x= dataset1.drop('Conversion',axis=1)\n",
    "dep_y=dataset1['Conversion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5dfecf75-34a3-45e3-9cdb-4cba2e5dd0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting GridSearchCV for LogisticRegression...\n",
      "Fitting GridSearchCV for RandomForestClassifier...\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Fitting GridSearchCV for DecisionTreeClassifier...\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Fitting GridSearchCV for XGBClassifier...\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[0.04391851, 0.08803141, 2.39901653, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.15572507, 0.18272468, 2.91713775, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.27749037, 0.07642272, 8.2236191 , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.05652592, 0.13382612, 2.85324058, ..., 0.        , 1.        ,\n",
       "         0.        ],\n",
       "        [0.02396129, 0.13838618, 1.00296447, ..., 0.        , 1.        ,\n",
       "         0.        ],\n",
       "        [0.1856701 , 0.05722808, 6.96473936, ..., 0.        , 0.        ,\n",
       "         0.        ]]),\n",
       " array([[1.36912000e+05, 6.49787007e+03, 4.39185107e-02, ...,\n",
       "         9.00000000e+00, 4.00000000e+00, 6.88000000e+02],\n",
       "        [4.17600000e+04, 3.89866861e+03, 1.55725071e-01, ...,\n",
       "         7.00000000e+00, 2.00000000e+00, 3.45900000e+03],\n",
       "        [8.84560000e+04, 1.54642960e+03, 2.77490369e-01, ...,\n",
       "         2.00000000e+00, 8.00000000e+00, 2.33700000e+03],\n",
       "        ...,\n",
       "        [1.25471000e+05, 4.60953464e+03, 5.65259173e-02, ...,\n",
       "         0.00000000e+00, 3.00000000e+00, 7.38000000e+02],\n",
       "        [1.07862000e+05, 9.47610635e+03, 2.39612921e-02, ...,\n",
       "         5.00000000e+00, 7.00000000e+00, 2.70900000e+03],\n",
       "        [9.30020000e+04, 7.74362707e+03, 1.85670096e-01, ...,\n",
       "         9.00000000e+00, 9.00000000e+00, 3.41000000e+02]]),\n",
       " array([[5.60000000e+01, 1.36912000e+05, 6.49787007e+03, ...,\n",
       "         9.00000000e+00, 4.00000000e+00, 6.88000000e+02],\n",
       "        [6.90000000e+01, 4.17600000e+04, 3.89866861e+03, ...,\n",
       "         7.00000000e+00, 2.00000000e+00, 3.45900000e+03],\n",
       "        [4.60000000e+01, 8.84560000e+04, 1.54642960e+03, ...,\n",
       "         2.00000000e+00, 8.00000000e+00, 2.33700000e+03],\n",
       "        ...,\n",
       "        [2.80000000e+01, 1.25471000e+05, 4.60953464e+03, ...,\n",
       "         0.00000000e+00, 3.00000000e+00, 7.38000000e+02],\n",
       "        [1.90000000e+01, 1.07862000e+05, 9.47610635e+03, ...,\n",
       "         5.00000000e+00, 7.00000000e+00, 2.70900000e+03],\n",
       "        [3.10000000e+01, 9.30020000e+04, 7.74362707e+03, ...,\n",
       "         9.00000000e+00, 9.00000000e+00, 3.41000000e+02]]),\n",
       " array([[6.49787007e+03, 4.39185107e-02, 8.80314121e-02, ...,\n",
       "         6.88000000e+02, 0.00000000e+00, 0.00000000e+00],\n",
       "        [3.89866861e+03, 1.55725071e-01, 1.82724683e-01, ...,\n",
       "         3.45900000e+03, 1.00000000e+00, 0.00000000e+00],\n",
       "        [1.54642960e+03, 2.77490369e-01, 7.64227201e-02, ...,\n",
       "         2.33700000e+03, 0.00000000e+00, 0.00000000e+00],\n",
       "        ...,\n",
       "        [4.60953464e+03, 5.65259173e-02, 1.33826123e-01, ...,\n",
       "         7.38000000e+02, 0.00000000e+00, 0.00000000e+00],\n",
       "        [9.47610635e+03, 2.39612921e-02, 1.38386181e-01, ...,\n",
       "         2.70900000e+03, 0.00000000e+00, 0.00000000e+00],\n",
       "        [7.74362707e+03, 1.85670096e-01, 5.72280791e-02, ...,\n",
       "         3.41000000e+02, 0.00000000e+00, 0.00000000e+00]])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rfelist=rfeFeature(indep_x,dep_y,12)\n",
    "rfelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5268b3ee-5557-4f40-831c-2395fb176d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification Report:\n",
      "[[137  57]\n",
      " [428 978]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.71      0.36       194\n",
      "           1       0.94      0.70      0.80      1406\n",
      "\n",
      "    accuracy                           0.70      1600\n",
      "   macro avg       0.59      0.70      0.58      1600\n",
      "weighted avg       0.86      0.70      0.75      1600\n",
      "\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "\n",
      " Decision Tree Classification Report:\n",
      "[[  63  131]\n",
      " [ 140 1266]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.32      0.32       194\n",
      "           1       0.91      0.90      0.90      1406\n",
      "\n",
      "    accuracy                           0.83      1600\n",
      "   macro avg       0.61      0.61      0.61      1600\n",
      "weighted avg       0.83      0.83      0.83      1600\n",
      "\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      " Random Forest Classification Report:\n",
      "[[  56  138]\n",
      " [  40 1366]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.29      0.39       194\n",
      "           1       0.91      0.97      0.94      1406\n",
      "\n",
      "    accuracy                           0.89      1600\n",
      "   macro avg       0.75      0.63      0.66      1600\n",
      "weighted avg       0.87      0.89      0.87      1600\n",
      "\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "\n",
      " XgboostClassification Report:\n",
      "[[  69  125]\n",
      " [ 102 1304]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.36      0.38       194\n",
      "           1       0.91      0.93      0.92      1406\n",
      "\n",
      "    accuracy                           0.86      1600\n",
      "   macro avg       0.66      0.64      0.65      1600\n",
      "weighted avg       0.85      0.86      0.85      1600\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "[[ 143   51]\n",
      " [ 386 1020]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.74      0.40       194\n",
      "           1       0.95      0.73      0.82      1406\n",
      "\n",
      "    accuracy                           0.73      1600\n",
      "   macro avg       0.61      0.73      0.61      1600\n",
      "weighted avg       0.87      0.73      0.77      1600\n",
      "\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "\n",
      " Decision Tree Classification Report:\n",
      "[[  55  139]\n",
      " [ 140 1266]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.28      0.28       194\n",
      "           1       0.90      0.90      0.90      1406\n",
      "\n",
      "    accuracy                           0.83      1600\n",
      "   macro avg       0.59      0.59      0.59      1600\n",
      "weighted avg       0.83      0.83      0.83      1600\n",
      "\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      " Random Forest Classification Report:\n",
      "[[  58  136]\n",
      " [  17 1389]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.30      0.43       194\n",
      "           1       0.91      0.99      0.95      1406\n",
      "\n",
      "    accuracy                           0.90      1600\n",
      "   macro avg       0.84      0.64      0.69      1600\n",
      "weighted avg       0.89      0.90      0.89      1600\n",
      "\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "\n",
      " XgboostClassification Report:\n",
      "[[  92  102]\n",
      " [  58 1348]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.47      0.53       194\n",
      "           1       0.93      0.96      0.94      1406\n",
      "\n",
      "    accuracy                           0.90      1600\n",
      "   macro avg       0.77      0.72      0.74      1600\n",
      "weighted avg       0.89      0.90      0.89      1600\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "[[ 144   50]\n",
      " [ 388 1018]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.74      0.40       194\n",
      "           1       0.95      0.72      0.82      1406\n",
      "\n",
      "    accuracy                           0.73      1600\n",
      "   macro avg       0.61      0.73      0.61      1600\n",
      "weighted avg       0.87      0.73      0.77      1600\n",
      "\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "\n",
      " Decision Tree Classification Report:\n",
      "[[  57  137]\n",
      " [ 124 1282]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.29      0.30       194\n",
      "           1       0.90      0.91      0.91      1406\n",
      "\n",
      "    accuracy                           0.84      1600\n",
      "   macro avg       0.61      0.60      0.61      1600\n",
      "weighted avg       0.83      0.84      0.83      1600\n",
      "\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      " Random Forest Classification Report:\n",
      "[[  57  137]\n",
      " [  22 1384]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.29      0.42       194\n",
      "           1       0.91      0.98      0.95      1406\n",
      "\n",
      "    accuracy                           0.90      1600\n",
      "   macro avg       0.82      0.64      0.68      1600\n",
      "weighted avg       0.89      0.90      0.88      1600\n",
      "\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "\n",
      " XgboostClassification Report:\n",
      "[[  97   97]\n",
      " [  60 1346]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.50      0.55       194\n",
      "           1       0.93      0.96      0.94      1406\n",
      "\n",
      "    accuracy                           0.90      1600\n",
      "   macro avg       0.78      0.73      0.75      1600\n",
      "weighted avg       0.89      0.90      0.90      1600\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "[[ 144   50]\n",
      " [ 388 1018]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.74      0.40       194\n",
      "           1       0.95      0.72      0.82      1406\n",
      "\n",
      "    accuracy                           0.73      1600\n",
      "   macro avg       0.61      0.73      0.61      1600\n",
      "weighted avg       0.87      0.73      0.77      1600\n",
      "\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "\n",
      " Decision Tree Classification Report:\n",
      "[[  54  140]\n",
      " [ 118 1288]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.28      0.30       194\n",
      "           1       0.90      0.92      0.91      1406\n",
      "\n",
      "    accuracy                           0.84      1600\n",
      "   macro avg       0.61      0.60      0.60      1600\n",
      "weighted avg       0.83      0.84      0.83      1600\n",
      "\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      " Random Forest Classification Report:\n",
      "[[  65  129]\n",
      " [  21 1385]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.34      0.46       194\n",
      "           1       0.91      0.99      0.95      1406\n",
      "\n",
      "    accuracy                           0.91      1600\n",
      "   macro avg       0.84      0.66      0.71      1600\n",
      "weighted avg       0.90      0.91      0.89      1600\n",
      "\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "\n",
      " XgboostClassification Report:\n",
      "[[ 101   93]\n",
      " [  57 1349]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.52      0.57       194\n",
      "           1       0.94      0.96      0.95      1406\n",
      "\n",
      "    accuracy                           0.91      1600\n",
      "   macro avg       0.79      0.74      0.76      1600\n",
      "weighted avg       0.90      0.91      0.90      1600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#12\n",
    "for i in rfelist:   \n",
    "    X_train, X_test, y_train, y_test=split_scalar(i,dep_y)   \n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "    \n",
    "        \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=logistic(X_train_resampled,y_train_resampled,X_test)\n",
    "    print(\"Logistic Regression Classification Report:\")\n",
    "    print(cm)\n",
    "    print(report)\n",
    "    \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=Decision(X_train_resampled,y_train_resampled,X_test)\n",
    "    print(\"\\n Decision Tree Classification Report:\")\n",
    "    print(cm)\n",
    "    print(report)\n",
    "    \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=random(X_train_resampled,y_train_resampled,X_test)\n",
    "    print(\"\\n Random Forest Classification Report:\")\n",
    "    print(cm)\n",
    "    print(report)\n",
    "    \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=Xgboost(X_train_resampled,y_train_resampled,X_test)\n",
    "    print(\"\\n XgboostClassification Report:\")\n",
    "    print(cm)\n",
    "    print(report)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b19bdca-56ac-4719-8b35-579f5a1bc350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification Report:\n",
      "[[137  57]\n",
      " [425 981]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.71      0.36       194\n",
      "           1       0.95      0.70      0.80      1406\n",
      "\n",
      "    accuracy                           0.70      1600\n",
      "   macro avg       0.59      0.70      0.58      1600\n",
      "weighted avg       0.86      0.70      0.75      1600\n",
      "\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "\n",
      " Decision Tree Classification Report:\n",
      "[[  63  131]\n",
      " [ 217 1189]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.32      0.27       194\n",
      "           1       0.90      0.85      0.87      1406\n",
      "\n",
      "    accuracy                           0.78      1600\n",
      "   macro avg       0.56      0.59      0.57      1600\n",
      "weighted avg       0.82      0.78      0.80      1600\n",
      "\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      " Random Forest Classification Report:\n",
      "[[  45  149]\n",
      " [  50 1356]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.23      0.31       194\n",
      "           1       0.90      0.96      0.93      1406\n",
      "\n",
      "    accuracy                           0.88      1600\n",
      "   macro avg       0.69      0.60      0.62      1600\n",
      "weighted avg       0.85      0.88      0.86      1600\n",
      "\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "\n",
      " XgboostClassification Report:\n",
      "[[  52  142]\n",
      " [  28 1378]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.27      0.38       194\n",
      "           1       0.91      0.98      0.94      1406\n",
      "\n",
      "    accuracy                           0.89      1600\n",
      "   macro avg       0.78      0.62      0.66      1600\n",
      "weighted avg       0.88      0.89      0.87      1600\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "[[ 142   52]\n",
      " [ 374 1032]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.73      0.40       194\n",
      "           1       0.95      0.73      0.83      1406\n",
      "\n",
      "    accuracy                           0.73      1600\n",
      "   macro avg       0.61      0.73      0.61      1600\n",
      "weighted avg       0.87      0.73      0.78      1600\n",
      "\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "\n",
      " Decision Tree Classification Report:\n",
      "[[  67  127]\n",
      " [ 171 1235]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.35      0.31       194\n",
      "           1       0.91      0.88      0.89      1406\n",
      "\n",
      "    accuracy                           0.81      1600\n",
      "   macro avg       0.59      0.61      0.60      1600\n",
      "weighted avg       0.83      0.81      0.82      1600\n",
      "\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      " Random Forest Classification Report:\n",
      "[[  48  146]\n",
      " [  36 1370]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.25      0.35       194\n",
      "           1       0.90      0.97      0.94      1406\n",
      "\n",
      "    accuracy                           0.89      1600\n",
      "   macro avg       0.74      0.61      0.64      1600\n",
      "weighted avg       0.86      0.89      0.87      1600\n",
      "\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "\n",
      " XgboostClassification Report:\n",
      "[[  69  125]\n",
      " [  31 1375]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.36      0.47       194\n",
      "           1       0.92      0.98      0.95      1406\n",
      "\n",
      "    accuracy                           0.90      1600\n",
      "   macro avg       0.80      0.67      0.71      1600\n",
      "weighted avg       0.89      0.90      0.89      1600\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "[[ 146   48]\n",
      " [ 377 1029]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.75      0.41       194\n",
      "           1       0.96      0.73      0.83      1406\n",
      "\n",
      "    accuracy                           0.73      1600\n",
      "   macro avg       0.62      0.74      0.62      1600\n",
      "weighted avg       0.87      0.73      0.78      1600\n",
      "\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "\n",
      " Decision Tree Classification Report:\n",
      "[[  62  132]\n",
      " [ 163 1243]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.32      0.30       194\n",
      "           1       0.90      0.88      0.89      1406\n",
      "\n",
      "    accuracy                           0.82      1600\n",
      "   macro avg       0.59      0.60      0.59      1600\n",
      "weighted avg       0.83      0.82      0.82      1600\n",
      "\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      " Random Forest Classification Report:\n",
      "[[  47  147]\n",
      " [  32 1374]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.24      0.34       194\n",
      "           1       0.90      0.98      0.94      1406\n",
      "\n",
      "    accuracy                           0.89      1600\n",
      "   macro avg       0.75      0.61      0.64      1600\n",
      "weighted avg       0.87      0.89      0.87      1600\n",
      "\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "\n",
      " XgboostClassification Report:\n",
      "[[  68  126]\n",
      " [  32 1374]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.35      0.46       194\n",
      "           1       0.92      0.98      0.95      1406\n",
      "\n",
      "    accuracy                           0.90      1600\n",
      "   macro avg       0.80      0.66      0.70      1600\n",
      "weighted avg       0.89      0.90      0.89      1600\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "[[ 142   52]\n",
      " [ 384 1022]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.73      0.39       194\n",
      "           1       0.95      0.73      0.82      1406\n",
      "\n",
      "    accuracy                           0.73      1600\n",
      "   macro avg       0.61      0.73      0.61      1600\n",
      "weighted avg       0.87      0.73      0.77      1600\n",
      "\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "\n",
      " Decision Tree Classification Report:\n",
      "[[  74  120]\n",
      " [ 212 1194]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.38      0.31       194\n",
      "           1       0.91      0.85      0.88      1406\n",
      "\n",
      "    accuracy                           0.79      1600\n",
      "   macro avg       0.58      0.62      0.59      1600\n",
      "weighted avg       0.83      0.79      0.81      1600\n",
      "\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      " Random Forest Classification Report:\n",
      "[[  63  131]\n",
      " [  50 1356]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.32      0.41       194\n",
      "           1       0.91      0.96      0.94      1406\n",
      "\n",
      "    accuracy                           0.89      1600\n",
      "   macro avg       0.73      0.64      0.67      1600\n",
      "weighted avg       0.87      0.89      0.87      1600\n",
      "\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "\n",
      " XgboostClassification Report:\n",
      "[[  79  115]\n",
      " [  28 1378]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.41      0.52       194\n",
      "           1       0.92      0.98      0.95      1406\n",
      "\n",
      "    accuracy                           0.91      1600\n",
      "   macro avg       0.83      0.69      0.74      1600\n",
      "weighted avg       0.90      0.91      0.90      1600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#12\n",
    "for i in rfelist:   \n",
    "    X_train, X_test, y_train, y_test=split_scalar(i,dep_y)\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    smote = SMOTE(sampling_strategy=1,random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "        \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=logistic(X_train_resampled,y_train_resampled,X_test)\n",
    "    print(\"Logistic Regression Classification Report:\")\n",
    "    print(cm)\n",
    "    print(report)\n",
    "    \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=Decision(X_train_resampled,y_train_resampled,X_test)\n",
    "    print(\"\\n Decision Tree Classification Report:\")\n",
    "    print(cm)\n",
    "    print(report)\n",
    "    \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=random(X_train_resampled,y_train_resampled,X_test)\n",
    "    print(\"\\n Random Forest Classification Report:\")\n",
    "    print(cm)\n",
    "    print(report)\n",
    "    \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=Xgboost(X_train_resampled,y_train_resampled,X_test)\n",
    "    print(\"\\n XgboostClassification Report:\")\n",
    "    print(cm)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2dce528b-c861-4f29-85d3-2ca197f16753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification Report:\n",
      "[[141  53]\n",
      " [454 952]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.73      0.36       194\n",
      "           1       0.95      0.68      0.79      1406\n",
      "\n",
      "    accuracy                           0.68      1600\n",
      "   macro avg       0.59      0.70      0.57      1600\n",
      "weighted avg       0.86      0.68      0.74      1600\n",
      "\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "\n",
      " Decision Tree Classification Report:\n",
      "[[  65  129]\n",
      " [ 176 1230]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.34      0.30       194\n",
      "           1       0.91      0.87      0.89      1406\n",
      "\n",
      "    accuracy                           0.81      1600\n",
      "   macro avg       0.59      0.60      0.59      1600\n",
      "weighted avg       0.83      0.81      0.82      1600\n",
      "\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      " Random Forest Classification Report:\n",
      "[[  43  151]\n",
      " [  40 1366]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.22      0.31       194\n",
      "           1       0.90      0.97      0.93      1406\n",
      "\n",
      "    accuracy                           0.88      1600\n",
      "   macro avg       0.71      0.60      0.62      1600\n",
      "weighted avg       0.85      0.88      0.86      1600\n",
      "\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "\n",
      " XgboostClassification Report:\n",
      "[[  51  143]\n",
      " [  33 1373]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.26      0.37       194\n",
      "           1       0.91      0.98      0.94      1406\n",
      "\n",
      "    accuracy                           0.89      1600\n",
      "   macro avg       0.76      0.62      0.65      1600\n",
      "weighted avg       0.87      0.89      0.87      1600\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "[[ 144   50]\n",
      " [ 376 1030]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.74      0.40       194\n",
      "           1       0.95      0.73      0.83      1406\n",
      "\n",
      "    accuracy                           0.73      1600\n",
      "   macro avg       0.62      0.74      0.62      1600\n",
      "weighted avg       0.87      0.73      0.78      1600\n",
      "\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "\n",
      " Decision Tree Classification Report:\n",
      "[[  53  141]\n",
      " [ 176 1230]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.27      0.25       194\n",
      "           1       0.90      0.87      0.89      1406\n",
      "\n",
      "    accuracy                           0.80      1600\n",
      "   macro avg       0.56      0.57      0.57      1600\n",
      "weighted avg       0.82      0.80      0.81      1600\n",
      "\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      " Random Forest Classification Report:\n",
      "[[  44  150]\n",
      " [  39 1367]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.23      0.32       194\n",
      "           1       0.90      0.97      0.94      1406\n",
      "\n",
      "    accuracy                           0.88      1600\n",
      "   macro avg       0.72      0.60      0.63      1600\n",
      "weighted avg       0.86      0.88      0.86      1600\n",
      "\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "\n",
      " XgboostClassification Report:\n",
      "[[  72  122]\n",
      " [  25 1381]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.37      0.49       194\n",
      "           1       0.92      0.98      0.95      1406\n",
      "\n",
      "    accuracy                           0.91      1600\n",
      "   macro avg       0.83      0.68      0.72      1600\n",
      "weighted avg       0.90      0.91      0.89      1600\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "[[ 144   50]\n",
      " [ 375 1031]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.74      0.40       194\n",
      "           1       0.95      0.73      0.83      1406\n",
      "\n",
      "    accuracy                           0.73      1600\n",
      "   macro avg       0.62      0.74      0.62      1600\n",
      "weighted avg       0.87      0.73      0.78      1600\n",
      "\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "\n",
      " Decision Tree Classification Report:\n",
      "[[  54  140]\n",
      " [ 159 1247]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.28      0.27       194\n",
      "           1       0.90      0.89      0.89      1406\n",
      "\n",
      "    accuracy                           0.81      1600\n",
      "   macro avg       0.58      0.58      0.58      1600\n",
      "weighted avg       0.82      0.81      0.82      1600\n",
      "\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      " Random Forest Classification Report:\n",
      "[[  38  156]\n",
      " [  36 1370]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.20      0.28       194\n",
      "           1       0.90      0.97      0.93      1406\n",
      "\n",
      "    accuracy                           0.88      1600\n",
      "   macro avg       0.71      0.59      0.61      1600\n",
      "weighted avg       0.85      0.88      0.86      1600\n",
      "\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "\n",
      " XgboostClassification Report:\n",
      "[[  67  127]\n",
      " [  26 1380]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.35      0.47       194\n",
      "           1       0.92      0.98      0.95      1406\n",
      "\n",
      "    accuracy                           0.90      1600\n",
      "   macro avg       0.82      0.66      0.71      1600\n",
      "weighted avg       0.89      0.90      0.89      1600\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "[[ 143   51]\n",
      " [ 392 1014]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.74      0.39       194\n",
      "           1       0.95      0.72      0.82      1406\n",
      "\n",
      "    accuracy                           0.72      1600\n",
      "   macro avg       0.61      0.73      0.61      1600\n",
      "weighted avg       0.87      0.72      0.77      1600\n",
      "\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "\n",
      " Decision Tree Classification Report:\n",
      "[[  76  118]\n",
      " [ 188 1218]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.39      0.33       194\n",
      "           1       0.91      0.87      0.89      1406\n",
      "\n",
      "    accuracy                           0.81      1600\n",
      "   macro avg       0.60      0.63      0.61      1600\n",
      "weighted avg       0.84      0.81      0.82      1600\n",
      "\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      " Random Forest Classification Report:\n",
      "[[  64  130]\n",
      " [  51 1355]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.33      0.41       194\n",
      "           1       0.91      0.96      0.94      1406\n",
      "\n",
      "    accuracy                           0.89      1600\n",
      "   macro avg       0.73      0.65      0.68      1600\n",
      "weighted avg       0.87      0.89      0.87      1600\n",
      "\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "\n",
      " XgboostClassification Report:\n",
      "[[  82  112]\n",
      " [  25 1381]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.42      0.54       194\n",
      "           1       0.92      0.98      0.95      1406\n",
      "\n",
      "    accuracy                           0.91      1600\n",
      "   macro avg       0.85      0.70      0.75      1600\n",
      "weighted avg       0.91      0.91      0.90      1600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#12\n",
    "for i in rfelist:   \n",
    "    X_train, X_test, y_train, y_test=split_scalar(i,dep_y)\n",
    "    from imblearn.over_sampling import ADASYN\n",
    "    adasyn = ADASYN(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = adasyn.fit_resample(X_train, y_train)\n",
    "    \n",
    "        \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=logistic(X_train_resampled,y_train_resampled,X_test)\n",
    "    print(\"Logistic Regression Classification Report:\")\n",
    "    print(cm)\n",
    "    print(report)\n",
    "    \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=Decision(X_train_resampled,y_train_resampled,X_test)\n",
    "    print(\"\\n Decision Tree Classification Report:\")\n",
    "    print(cm)\n",
    "    print(report)\n",
    "    \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=random(X_train_resampled,y_train_resampled,X_test)\n",
    "    print(\"\\n Random Forest Classification Report:\")\n",
    "    print(cm)\n",
    "    print(report)\n",
    "    \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=Xgboost(X_train_resampled,y_train_resampled,X_test)\n",
    "    print(\"\\n XgboostClassification Report:\")\n",
    "    print(cm)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5c452c5-dc41-46fd-a555-e7f41d3dae7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification Report:\n",
      "[[ 123   71]\n",
      " [ 276 1130]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.63      0.41       194\n",
      "           1       0.94      0.80      0.87      1406\n",
      "\n",
      "    accuracy                           0.78      1600\n",
      "   macro avg       0.62      0.72      0.64      1600\n",
      "weighted avg       0.86      0.78      0.81      1600\n",
      "\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "\n",
      " Decision Tree Classification Report:\n",
      "[[  71  123]\n",
      " [ 177 1229]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.37      0.32       194\n",
      "           1       0.91      0.87      0.89      1406\n",
      "\n",
      "    accuracy                           0.81      1600\n",
      "   macro avg       0.60      0.62      0.61      1600\n",
      "weighted avg       0.83      0.81      0.82      1600\n",
      "\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      " Random Forest Classification Report:\n",
      "[[  42  152]\n",
      " [  42 1364]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.22      0.30       194\n",
      "           1       0.90      0.97      0.93      1406\n",
      "\n",
      "    accuracy                           0.88      1600\n",
      "   macro avg       0.70      0.59      0.62      1600\n",
      "weighted avg       0.85      0.88      0.86      1600\n",
      "\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "\n",
      " XgboostClassification Report:\n",
      "[[  59  135]\n",
      " [  36 1370]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.30      0.41       194\n",
      "           1       0.91      0.97      0.94      1406\n",
      "\n",
      "    accuracy                           0.89      1600\n",
      "   macro avg       0.77      0.64      0.67      1600\n",
      "weighted avg       0.88      0.89      0.88      1600\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "[[ 129   65]\n",
      " [ 264 1142]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.66      0.44       194\n",
      "           1       0.95      0.81      0.87      1406\n",
      "\n",
      "    accuracy                           0.79      1600\n",
      "   macro avg       0.64      0.74      0.66      1600\n",
      "weighted avg       0.87      0.79      0.82      1600\n",
      "\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "\n",
      " Decision Tree Classification Report:\n",
      "[[  72  122]\n",
      " [ 203 1203]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.37      0.31       194\n",
      "           1       0.91      0.86      0.88      1406\n",
      "\n",
      "    accuracy                           0.80      1600\n",
      "   macro avg       0.58      0.61      0.59      1600\n",
      "weighted avg       0.83      0.80      0.81      1600\n",
      "\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      " Random Forest Classification Report:\n",
      "[[  66  128]\n",
      " [  56 1350]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.34      0.42       194\n",
      "           1       0.91      0.96      0.94      1406\n",
      "\n",
      "    accuracy                           0.89      1600\n",
      "   macro avg       0.73      0.65      0.68      1600\n",
      "weighted avg       0.87      0.89      0.87      1600\n",
      "\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "\n",
      " XgboostClassification Report:\n",
      "[[  69  125]\n",
      " [  27 1379]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.36      0.48       194\n",
      "           1       0.92      0.98      0.95      1406\n",
      "\n",
      "    accuracy                           0.91      1600\n",
      "   macro avg       0.82      0.67      0.71      1600\n",
      "weighted avg       0.89      0.91      0.89      1600\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "[[ 127   67]\n",
      " [ 265 1141]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.65      0.43       194\n",
      "           1       0.94      0.81      0.87      1406\n",
      "\n",
      "    accuracy                           0.79      1600\n",
      "   macro avg       0.63      0.73      0.65      1600\n",
      "weighted avg       0.87      0.79      0.82      1600\n",
      "\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "\n",
      " Decision Tree Classification Report:\n",
      "[[  72  122]\n",
      " [ 187 1219]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.37      0.32       194\n",
      "           1       0.91      0.87      0.89      1406\n",
      "\n",
      "    accuracy                           0.81      1600\n",
      "   macro avg       0.59      0.62      0.60      1600\n",
      "weighted avg       0.83      0.81      0.82      1600\n",
      "\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      " Random Forest Classification Report:\n",
      "[[  68  126]\n",
      " [  56 1350]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.35      0.43       194\n",
      "           1       0.91      0.96      0.94      1406\n",
      "\n",
      "    accuracy                           0.89      1600\n",
      "   macro avg       0.73      0.66      0.68      1600\n",
      "weighted avg       0.87      0.89      0.88      1600\n",
      "\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "\n",
      " XgboostClassification Report:\n",
      "[[  70  124]\n",
      " [  34 1372]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.36      0.47       194\n",
      "           1       0.92      0.98      0.95      1406\n",
      "\n",
      "    accuracy                           0.90      1600\n",
      "   macro avg       0.80      0.67      0.71      1600\n",
      "weighted avg       0.89      0.90      0.89      1600\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "[[ 128   66]\n",
      " [ 254 1152]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.66      0.44       194\n",
      "           1       0.95      0.82      0.88      1406\n",
      "\n",
      "    accuracy                           0.80      1600\n",
      "   macro avg       0.64      0.74      0.66      1600\n",
      "weighted avg       0.87      0.80      0.83      1600\n",
      "\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "\n",
      " Decision Tree Classification Report:\n",
      "[[  79  115]\n",
      " [ 187 1219]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.41      0.34       194\n",
      "           1       0.91      0.87      0.89      1406\n",
      "\n",
      "    accuracy                           0.81      1600\n",
      "   macro avg       0.61      0.64      0.62      1600\n",
      "weighted avg       0.84      0.81      0.82      1600\n",
      "\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      " Random Forest Classification Report:\n",
      "[[  63  131]\n",
      " [  48 1358]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.32      0.41       194\n",
      "           1       0.91      0.97      0.94      1406\n",
      "\n",
      "    accuracy                           0.89      1600\n",
      "   macro avg       0.74      0.65      0.68      1600\n",
      "weighted avg       0.87      0.89      0.87      1600\n",
      "\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "\n",
      " XgboostClassification Report:\n",
      "[[  80  114]\n",
      " [  23 1383]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.41      0.54       194\n",
      "           1       0.92      0.98      0.95      1406\n",
      "\n",
      "    accuracy                           0.91      1600\n",
      "   macro avg       0.85      0.70      0.75      1600\n",
      "weighted avg       0.91      0.91      0.90      1600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#12\n",
    "for i in rfelist:   \n",
    "    X_train, X_test, y_train, y_test=split_scalar(i,dep_y)\n",
    "    from imblearn.over_sampling import BorderlineSMOTE\n",
    "    smote_border = BorderlineSMOTE(sampling_strategy=0.75, random_state=42, kind='borderline-1')\n",
    "    X_train_resampled, y_train_resampled = smote_border.fit_resample(X_train, y_train)\n",
    "\n",
    "    \n",
    "        \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=logistic(X_train_resampled,y_train_resampled,X_test)\n",
    "    print(\"Logistic Regression Classification Report:\")\n",
    "    print(cm)\n",
    "    print(report)\n",
    "    \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=Decision(X_train_resampled,y_train_resampled,X_test)\n",
    "    print(\"\\n Decision Tree Classification Report:\")\n",
    "    print(cm)\n",
    "    print(report)\n",
    "    \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=random(X_train_resampled,y_train_resampled,X_test)\n",
    "    print(\"\\n Random Forest Classification Report:\")\n",
    "    print(cm)\n",
    "    print(report)\n",
    "    \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=Xgboost(X_train_resampled,y_train_resampled,X_test)\n",
    "    print(\"\\n XgboostClassification Report:\")\n",
    "    print(cm)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8171b6c0-18a4-4f80-b432-a65de57b3285",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
